# Linear Regression Algorithm - Advertising Project

## üìö Overview

Linear Regression is a fundamental supervised learning algorithm used to model the linear relationship between input features (independent variables) and a continuous output (dependent variable). In the Advertising project, it predicts sales revenue based on advertising spending across TV, Radio, and Newspaper channels.

---

## üéØ Algorithm Significance

### Why Linear Regression Matters

1. **Interpretability**: Coefficients directly show the impact of each feature on the target
2. **Simplicity**: Easy to understand and implement
3. **Efficiency**: Fast training and prediction
4. **Baseline Model**: Provides baseline performance for comparison
5. **Real-world Application**: Widely used in business analytics and forecasting
6. **Mathematical Foundation**: Foundation for more complex algorithms

### Key Characteristics

- **Supervised Learning**: Requires labeled training data
- **Regression Task**: Predicts continuous numerical values
- **Linear Relationship**: Assumes linear relationship between features and target
- **Parametric Model**: Learns fixed number of parameters
- **Deterministic**: Same input always produces same output

---

## üîß How Linear Regression Works

### Conceptual Understanding

Linear Regression finds the best-fitting straight line (or hyperplane in multiple dimensions) through the data points that minimizes prediction errors.

```
Sales = Œ≤‚ÇÄ + Œ≤‚ÇÅ(TV) + Œ≤‚ÇÇ(Radio) + Œ≤‚ÇÉ(Newspaper) + Œµ

Where:
- Sales: Predicted sales revenue (target)
- Œ≤‚ÇÄ: Intercept (baseline sales when all features are 0)
- Œ≤‚ÇÅ, Œ≤‚ÇÇ, Œ≤‚ÇÉ: Coefficients (weights) for each feature
- TV, Radio, Newspaper: Input features (advertising spend)
- Œµ: Error term (residual)
```

### Step-by-Step Process

#### Step 1: Initialize Parameters
```
Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, Œ≤‚ÇÉ = random initial values or zeros
```

#### Step 2: Make Predictions
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + Œ≤‚ÇÉx‚ÇÉ

Where:
- ≈∑: Predicted value
- x‚ÇÅ, x‚ÇÇ, x‚ÇÉ: Feature values
```

#### Step 3: Calculate Error (Loss)
```
Error = y - ≈∑ (Residual for each sample)
```

#### Step 4: Optimize Parameters
Using Ordinary Least Squares (OLS) method to minimize sum of squared errors:

```
Minimize: SSE = Œ£(y·µ¢ - ≈∑·µ¢)¬≤

Solution: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy

Where:
- X: Feature matrix (including intercept column)
- y: Target vector
- Œ≤: Coefficient vector
```

#### Step 5: Evaluate Performance
Calculate metrics like MSE, RMSE, MAE, R¬≤

---

## üìê Mathematical Formulas

### 1. Linear Regression Model

**Simple Linear Regression (Single Feature)**:
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx
```

**Multiple Linear Regression (Multiple Features)**:
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô

In vector form:
≈∑ = XŒ≤

Where:
X = [1  x‚ÇÅ‚ÇÅ  x‚ÇÅ‚ÇÇ  ...  x‚ÇÅ‚Çô]
    [1  x‚ÇÇ‚ÇÅ  x‚ÇÇ‚ÇÇ  ...  x‚ÇÇ‚Çô]
    [‚ãÆ   ‚ãÆ    ‚ãÆ    ‚ã±   ‚ãÆ  ]
    [1  x‚Çò‚ÇÅ  x‚Çò‚ÇÇ  ...  x‚Çò‚Çô]

Œ≤ = [Œ≤‚ÇÄ]
    [Œ≤‚ÇÅ]
    [Œ≤‚ÇÇ]
    [‚ãÆ ]
    [Œ≤‚Çô]
```

### 2. Cost Function (Loss Function)

**Mean Squared Error (MSE)**:
```
MSE = (1/m) Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤

Where:
- m: Number of samples
- y·µ¢: Actual value
- ≈∑·µ¢: Predicted value
```

**Sum of Squared Errors (SSE)**:
```
SSE = Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤
```

### 3. Parameter Estimation (Ordinary Least Squares)

**Normal Equation**:
```
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy

Where:
- X·µÄ: Transpose of X
- (X·µÄX)‚Åª¬π: Inverse of (X·µÄX)
- y: Target vector
```

**Derivation**:
```
Minimize: L(Œ≤) = (y - XŒ≤)·µÄ(y - XŒ≤)

Taking derivative with respect to Œ≤:
‚àÇL/‚àÇŒ≤ = -2X·µÄ(y - XŒ≤) = 0

Solving for Œ≤:
X·µÄXŒ≤ = X·µÄy
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

### 4. Predictions

**Single Sample Prediction**:
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + Œ≤‚ÇÉx‚ÇÉ
```

**Multiple Sample Predictions**:
```
≈∑ = XŒ≤
```

### 5. Residuals

**Residual for Each Sample**:
```
e·µ¢ = y·µ¢ - ≈∑·µ¢
```

**Residual Sum of Squares**:
```
RSS = Œ£·µ¢‚Çå‚ÇÅ·µê e·µ¢¬≤ = Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤
```

### 6. Performance Metrics

**Mean Squared Error (MSE)**:
```
MSE = (1/m) Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤
```

**Root Mean Squared Error (RMSE)**:
```
RMSE = ‚àöMSE = ‚àö[(1/m) Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤]
```

**Mean Absolute Error (MAE)**:
```
MAE = (1/m) Œ£·µ¢‚Çå‚ÇÅ·µê |y·µ¢ - ≈∑·µ¢|
```

**R¬≤ Score (Coefficient of Determination)**:
```
R¬≤ = 1 - (SSres/SStot)

Where:
- SSres = Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - ≈∑·µ¢)¬≤  (Residual Sum of Squares)
- SStot = Œ£·µ¢‚Çå‚ÇÅ·µê (y·µ¢ - »≥)¬≤   (Total Sum of Squares)
- »≥ = (1/m) Œ£·µ¢‚Çå‚ÇÅ·µê y·µ¢        (Mean of actual values)

Interpretation:
- R¬≤ = 1: Perfect fit
- R¬≤ = 0: Model explains no variance
- R¬≤ < 0: Model worse than horizontal line
```

**Adjusted R¬≤** (accounts for number of features):
```
Adjusted R¬≤ = 1 - [(1 - R¬≤)(m - 1)/(m - p - 1)]

Where:
- m: Number of samples
- p: Number of features
```

### 7. Feature Importance

**Standardized Coefficients** (for feature importance):
```
Œ≤_standardized = Œ≤ √ó (œÉ‚Çì/œÉ·µß)

Where:
- œÉ‚Çì: Standard deviation of feature x
- œÉ·µß: Standard deviation of target y
```

---

## üéì Learning Perspective

### Assumptions of Linear Regression

1. **Linearity**: Relationship between features and target is linear
2. **Independence**: Observations are independent
3. **Homoscedasticity**: Constant variance of residuals
4. **Normality**: Residuals are normally distributed
5. **No Multicollinearity**: Features are not highly correlated

### Advantages

‚úì Simple and interpretable
‚úì Fast training and prediction
‚úì Works well with linear relationships
‚úì Provides confidence intervals
‚úì Computationally efficient
‚úì Good baseline model
‚úì Coefficients show feature impact

### Disadvantages

‚úó Assumes linear relationships
‚úó Sensitive to outliers
‚úó Assumes constant variance
‚úó Cannot capture non-linear patterns
‚úó Requires feature scaling for some algorithms
‚úó Assumes independence of features
‚úó May underfit complex relationships

---

## üéØ Why Linear Regression for Advertising Dataset

### Dataset Characteristics

```
Dataset: Advertising
- Samples: 200
- Features: 3 (TV, Radio, Newspaper)
- Target: Sales (continuous)
- Feature Type: All numerical
- Relationship: Appears linear
```

### Reasons for Selection

#### 1. **Linear Relationship in Data**
```
Correlation Analysis:
TV ‚Üî Sales:        0.78 (Strong positive)
Radio ‚Üî Sales:     0.58 (Moderate positive)
Newspaper ‚Üî Sales: 0.23 (Weak positive)

‚Üí Clear linear relationships visible
```

#### 2. **Problem Type**
- **Regression Task**: Predicting continuous sales values
- **Not Classification**: Not predicting categories
- **Linear Regression** is ideal for continuous prediction

#### 3. **Feature Characteristics**
- All features are numerical (no encoding needed)
- Features are continuous (not categorical)
- Features are on similar scales
- No complex interactions apparent

#### 4. **Dataset Size**
- 200 samples is sufficient for linear regression
- Not too small (would overfit easily)
- Not too large (computational efficiency)
- Good for learning purposes

#### 5. **Interpretability Requirement**
- Business stakeholders want to understand feature impact
- Linear Regression provides clear coefficient interpretation
- Easy to explain: "Each $1000 TV spend increases sales by $X"

#### 6. **Performance Expectations**
- Linear Regression achieves R¬≤ ‚âà 0.87-0.91
- Strong predictive power on this dataset
- Residuals show good distribution
- No obvious non-linear patterns

#### 7. **Computational Efficiency**
- Training time: < 1 second
- Prediction time: < 1 millisecond
- Memory efficient
- Suitable for real-time applications

#### 8. **Educational Value**
- Fundamental algorithm for learning ML
- Clear mathematical foundation
- Easy to understand and implement
- Good baseline for comparison

---

## üìä Advertising Dataset Analysis

### Feature-Target Relationships

```
TV Advertising:
- Range: 0.7 to 296.4 (thousands)
- Mean: 147.0
- Correlation with Sales: 0.78 (Strong)
- Interpretation: Strong positive linear relationship

Radio Advertising:
- Range: 0.0 to 49.6 (thousands)
- Mean: 23.3
- Correlation with Sales: 0.58 (Moderate)
- Interpretation: Moderate positive linear relationship

Newspaper Advertising:
- Range: 0.3 to 114.0 (thousands)
- Mean: 30.6
- Correlation with Sales: 0.23 (Weak)
- Interpretation: Weak positive linear relationship

Sales (Target):
- Range: 1.6 to 27.0 (thousands)
- Mean: 14.0
- Standard Deviation: 5.2
```

### Why Linear Model Fits Well

1. **Scatter Plot Analysis**: Points roughly follow a line
2. **Correlation Strength**: Strong correlations indicate linearity
3. **Residual Distribution**: Residuals appear randomly distributed
4. **No Obvious Patterns**: No curved or non-linear patterns visible
5. **Homoscedasticity**: Variance appears constant across range

---

## üî¨ Mathematical Derivation for Advertising

### Setting Up the Problem

**Model Equation**:
```
Sales = Œ≤‚ÇÄ + Œ≤‚ÇÅ(TV) + Œ≤‚ÇÇ(Radio) + Œ≤‚ÇÉ(Newspaper) + Œµ
```

**Matrix Form**:
```
y = XŒ≤ + Œµ

Where:
y = [Sales‚ÇÅ]      X = [1  TV‚ÇÅ  Radio‚ÇÅ  Newspaper‚ÇÅ]
    [Sales‚ÇÇ]          [1  TV‚ÇÇ  Radio‚ÇÇ  Newspaper‚ÇÇ]
    [‚ãÆ      ]         [‚ãÆ  ‚ãÆ    ‚ãÆ       ‚ãÆ          ]
    [Sales‚ÇÇ‚ÇÄ‚ÇÄ]        [1  TV‚ÇÇ‚ÇÄ‚ÇÄ Radio‚ÇÇ‚ÇÄ‚ÇÄ Newspaper‚ÇÇ‚ÇÄ‚ÇÄ]

Œ≤ = [Œ≤‚ÇÄ]
    [Œ≤‚ÇÅ]
    [Œ≤‚ÇÇ]
    [Œ≤‚ÇÉ]
```

### Solving for Coefficients

**Objective**: Minimize SSE = Œ£(y·µ¢ - ≈∑·µ¢)¬≤

**Solution**:
```
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy

Step 1: Calculate X·µÄX (4√ó4 matrix)
Step 2: Calculate inverse (X·µÄX)‚Åª¬π
Step 3: Calculate X·µÄy (4√ó1 vector)
Step 4: Multiply (X·µÄX)‚Åª¬πX·µÄy to get Œ≤
```

### Example Coefficients

```
Typical Results:
Œ≤‚ÇÄ = 6.97  (Intercept: baseline sales)
Œ≤‚ÇÅ = 0.046 (TV coefficient: each $1000 TV spend ‚Üí $46 sales increase)
Œ≤‚ÇÇ = 0.189 (Radio coefficient: each $1000 Radio spend ‚Üí $189 sales increase)
Œ≤‚ÇÉ = -0.001 (Newspaper coefficient: minimal negative impact)
```

### Making Predictions

**For a New Campaign**:
```
Input: TV = $100k, Radio = $30k, Newspaper = $20k

Prediction:
Sales = 6.97 + 0.046(100) + 0.189(30) + (-0.001)(20)
      = 6.97 + 4.6 + 5.67 - 0.02
      = 17.21 (thousands = $17,210)
```

---

## üîÑ Training Process

### Step 1: Data Preparation
```
1. Load 200 samples from GitHub
2. Extract features: X = [TV, Radio, Newspaper]
3. Extract target: y = [Sales]
4. Add intercept column to X
```

### Step 2: Train-Test Split
```
Total: 200 samples
Training: 134 samples (67%)
Testing: 66 samples (33%)
Random state: 3 (reproducibility)
```

### Step 3: Feature Scaling
```
StandardScaler:
- Calculate mean and std for each feature
- Transform: x_scaled = (x - mean) / std
- Apply to both training and testing data
```

### Step 4: Model Training
```
Using Normal Equation:
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy

Computational complexity: O(n¬≥) where n = number of features
For 3 features: Very fast computation
```

### Step 5: Evaluation
```
On Training Set:
- Calculate predictions: ≈∑ = XŒ≤
- Calculate metrics: MSE, RMSE, MAE, R¬≤

On Testing Set:
- Calculate predictions: ≈∑ = XŒ≤
- Calculate metrics: MSE, RMSE, MAE, R¬≤
- Compare with training metrics
```

---

## üìà Performance Analysis

### Expected Results

```
Training Metrics:
- R¬≤ Score: 0.906 (90.6% variance explained)
- RMSE: 1.51 (average error: $1,510)
- MAE: 1.18 (mean absolute error: $1,180)

Testing Metrics:
- R¬≤ Score: 0.872 (87.2% variance explained)
- RMSE: 2.04 (average error: $2,040)
- MAE: 1.40 (mean absolute error: $1,400)

Interpretation:
- Model explains 87% of variance in test data
- Average prediction error: ¬±$2,040
- Good generalization (small gap between train/test)
```

### Residual Analysis

```
Residuals = Actual - Predicted

Properties of Good Residuals:
‚úì Mean ‚âà 0 (no systematic bias)
‚úì Normally distributed
‚úì Constant variance (homoscedasticity)
‚úì No patterns or trends
‚úì Independent observations
```

---

## üéØ Practical Implications

### Business Insights

1. **TV Advertising**: Strongest impact on sales
   - Coefficient: 0.046
   - Interpretation: $1000 TV spend ‚Üí $46 sales increase

2. **Radio Advertising**: Moderate impact
   - Coefficient: 0.189
   - Interpretation: $1000 Radio spend ‚Üí $189 sales increase

3. **Newspaper Advertising**: Minimal impact
   - Coefficient: -0.001
   - Interpretation: Negligible or negative impact

### Decision Making

```
Budget Allocation Strategy:
- Prioritize TV advertising (highest ROI)
- Use Radio as secondary channel
- Minimize Newspaper spending
- Test different allocations using model
```

---

## üîó Comparison with Alternatives

### Why Not Other Algorithms?

| Algorithm | Reason Not Chosen |
|-----------|------------------|
| Polynomial Regression | No evidence of non-linear relationships |
| Ridge/Lasso Regression | No multicollinearity issues detected |
| Decision Trees | Overkill for linear data, less interpretable |
| Neural Networks | Too complex for simple linear relationship |
| SVM | Unnecessary for linear problem |

---

## üí° Key Takeaways

1. **Linear Regression** is ideal for predicting continuous values with linear relationships
2. **Advertising Dataset** shows clear linear relationships between features and sales
3. **Mathematical Foundation** based on minimizing sum of squared errors
4. **Interpretability** allows business stakeholders to understand feature impact
5. **Performance** is strong (R¬≤ ‚âà 0.87) with good generalization
6. **Efficiency** makes it suitable for real-time predictions
7. **Educational Value** as fundamental ML algorithm

---

## üìö Further Reading

### Concepts to Explore
- Regularization (Ridge, Lasso)
- Polynomial Regression
- Gradient Descent vs Normal Equation
- Feature Engineering
- Cross-validation
- Residual Analysis

### Resources
- [Scikit-learn Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [Linear Regression Mathematics](https://en.wikipedia.org/wiki/Linear_regression)
- [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)

---

**Last Updated**: January 2026
**Version**: 1.0
**Status**: Complete
