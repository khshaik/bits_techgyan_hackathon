# Titanic Survival Prediction - Decision Tree Classification

## üìä Project Overview

The Titanic Survival Prediction project demonstrates how to predict passenger survival outcomes using machine learning classification. This project uses Decision Tree Classifier to build a predictive model and provides both Jupyter Notebook and Python script implementations.

### üéØ Business Context

**Domain**: Classification & Survival Analysis

**Problem Statement**: 
Given passenger demographic information and ticket details from the Titanic disaster, predict whether a passenger survived or not. This classic dataset demonstrates fundamental ML classification concepts.

**Real-World Application**:
- Risk assessment and prediction
- Historical data analysis
- Classification model fundamentals
- Feature importance analysis
- Decision tree interpretation

---

## üìà Dataset Information

### Source
- **URL**: https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv
- **Format**: CSV (Comma-Separated Values)
- **Size**: 891 rows √ó 12 columns
- **Target Variable**: Survived (0 = Did not survive, 1 = Survived)

### Features

| Column | Type | Description | Missing % |
|--------|------|-------------|-----------|
| PassengerId | int64 | Unique passenger identifier | 0% |
| Survived | int64 | Survival outcome (target) | 0% |
| Pclass | int64 | Passenger class (1, 2, or 3) | 0% |
| Name | object | Passenger name | 0% |
| Sex | object | Gender (male/female) | 0% |
| Age | float64 | Age in years | 19.87% |
| SibSp | int64 | Number of siblings/spouses | 0% |
| Parch | int64 | Number of parents/children | 0% |
| Ticket | object | Ticket number | 0% |
| Fare | float64 | Ticket fare | 0% |
| Cabin | object | Cabin number | 77.10% |
| Embarked | object | Port of embarkation (C, Q, S) | 0.22% |

### Data Quality
- **Missing Values**: Age (177), Cabin (687), Embarked (2)
- **Duplicates**: None detected
- **Outliers**: Present but retained for analysis
- **Data Type Issues**: None

### Statistical Summary

```
       PassengerId    Survived      Pclass         Age       SibSp  \
count   891.000000  891.000000  891.000000  714.000000  891.000000   
mean    446.000000    0.383838    2.308642   29.699118    0.523008   
std     257.353842    0.486592    0.836071   14.526497    1.102743   
min       1.000000    0.000000    1.000000    0.420000    0.000000   
25%     223.500000    0.000000    2.000000   20.125000    0.000000   
50%     446.000000    0.000000    3.000000   28.000000    0.000000   
75%     668.500000    1.000000    3.000000   38.000000    1.000000   
max     891.000000    1.000000    3.000000   80.000000    8.000000   

            Parch        Fare  
count  891.000000  891.000000  
mean     0.381594   32.204208  
std      0.806057   49.693429  
min      0.000000    0.000000  
25%      0.000000    7.910400  
50%      0.000000   14.454200  
75%      0.000000   31.000000  
max      6.000000  512.329200  
```

### Survival Statistics

```
Total Passengers: 891
Survived: 342 (38.38%)
Did Not Survive: 549 (61.62%)

Survival by Passenger Class:
Class 1: 62.96% survival rate
Class 2: 47.28% survival rate
Class 3: 24.24% survival rate

Survival by Gender:
Female: 74.20% survival rate
Male: 18.89% survival rate
```

---

## ü§ñ Machine Learning Model

### Algorithm: Decision Tree Classifier

**Why Decision Tree?**
- Interpretable decision rules (easy to explain)
- Handles both numerical and categorical data
- No feature scaling required
- Captures non-linear relationships
- Provides feature importance rankings

### Model Configuration

```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(
    random_state=3,           # Reproducibility
    max_depth=5,              # Prevent overfitting
    min_samples_split=10,     # Minimum samples to split
    min_samples_leaf=5        # Minimum samples in leaf
)
```

### Training Parameters

| Parameter | Value | Reason |
|-----------|-------|--------|
| Test Size | 0.33 (33%) | Standard validation split |
| Training Size | 0.67 (67%) | Sufficient training data |
| Random State | 3 | Reproducibility |
| Max Depth | 5 | Prevent overfitting |
| Min Samples Split | 10 | Reduce noise |
| Min Samples Leaf | 5 | Ensure leaf stability |

### Feature Importance

Based on decision tree splits:

```
Feature      Importance    Visualization
Sex          ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (Highest - "women and children first")
Pclass       ‚òÖ‚òÖ‚òÖ‚òÖ (High - class-based survival)
Age          ‚òÖ‚òÖ‚òÖ (Moderate - age affects survival)
Fare         ‚òÖ‚òÖ (Low - ticket price indicator)
SibSp        ‚òÖ (Minimal - family size)
```

---

## üìä Model Performance

### Training Metrics

```
Accuracy Score:  78.64%
Precision:       72.03% (of predicted survivors, 72% actually survived)
Recall:          73.91% (of actual survivors, 74% were correctly predicted)
F1-Score:        72.96% (harmonic mean of precision and recall)
```

### Testing Metrics

```
Accuracy Score:  78.64%
Precision:       72.03%
Recall:          73.91%
F1-Score:        72.96%
```

### Confusion Matrix

```
                Predicted Did Not Survive    Predicted Survived
Actually Did Not Survive:  147 (TN)                33 (FP)
Actually Survived:         30 (FN)                85 (TP)

Interpretation:
- True Negatives (TN): 147 - Correctly predicted non-survivors
- False Positives (FP): 33 - Incorrectly predicted survivors
- False Negatives (FN): 30 - Incorrectly predicted non-survivors
- True Positives (TP): 85 - Correctly predicted survivors
```

### Performance Interpretation

- **Accuracy of 78.64%**: Model correctly predicts survival in 78.64% of cases
- **Balanced Precision/Recall**: Good balance between false positives and false negatives
- **F1-Score of 72.96%**: Strong overall performance on minority class (survivors)
- **Sensitivity (Recall) of 73.91%**: Catches 74% of actual survivors

---

## üèóÔ∏è Project Architecture

### File Structure

```
titanic/
‚îÇ
‚îú‚îÄ‚îÄ algorithm/
‚îÇ   ‚îî‚îÄ‚îÄ ALGORITHM_DECISION_TREE.md
‚îÇ
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ titanic_data_analysis.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ titanic_data_analysis.py
‚îÇ
‚îú‚îÄ‚îÄ notes/
‚îÇ   ‚îî‚îÄ‚îÄ TITANIC_JUPYTER_DEPLOYMENT.md
‚îÇ
‚îú‚îÄ‚îÄ setup/
‚îÇ   ‚îî‚îÄ‚îÄ SETUP.md
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_titanic_app.py
‚îÇ   ‚îî‚îÄ‚îÄ TESTING_SUMMARY.md
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

### Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Data Processing | Pandas | 2.0.3 |
| Numerical Computing | NumPy | 1.24.3 |
| ML Library | Scikit-learn | 1.3.0 |
| Visualization | Matplotlib | 3.7.2 |
| Statistical Viz | Seaborn | 0.12.2 |
| Notebooks | Jupyter | 1.0.0 |
| Python | Python | 3.7+ |

---

## üöÄ Quick Start

### Prerequisites
- Python 3.7 or higher
- pip (Python package manager)
- Virtual environment (recommended)

### Installation

```bash
# 1. Navigate to titanic directory
cd titanic

# 2. Create virtual environment
python3 -m venv venv

# 3. Activate virtual environment
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

# 4. Install dependencies
pip install -r requirements.txt

# 5a. Run as Jupyter Notebook
jupyter notebook BITS_AIML_Titanic_Jan3rd2026.ipynb

# 5b. OR run as Python script
python BITS_AIML_Titanic_Jan3rd2026.py
```

### Usage

**Jupyter Notebook**:
1. Open notebook in browser
2. Execute cells sequentially
3. Modify and experiment with code
4. View inline visualizations

**Python Script**:
1. Run from command line
2. View console output
3. Visualizations display in separate windows
4. Complete execution in batch mode

---

## üîÑ Workflow Steps

### Step 1: ANALYZE
**Objective**: Understand Titanic dataset structure

**Operations**:
- Load CSV from GitHub
- Display dataset dimensions (891 rows, 12 columns)
- Show column information and data types
- Calculate statistical summary
- Identify missing values

**Output**: Dataset overview with statistics

---

### Step 2: CLEAN
**Objective**: Prepare data for modeling

**Operations**:
- Handle missing Age values (fill with class-specific mean)
- Handle missing Embarked values (fill with mode 'S')
- Remove rows with remaining missing values
- Encode categorical variables (Sex, Embarked)
- Validate data quality

**Output**: Cleaned dataset ready for analysis

---

### Step 3: VISUALIZE
**Objective**: Explore survival patterns

**Visualizations**:
1. **Survival Distribution**: Count and percentage of survivors
2. **Survival by Class**: Class-based survival rates
3. **Survival by Gender**: Gender-based survival patterns
4. **Age Distribution**: Age vs survival relationship

**Key Insights**:
- "Women and children first" policy evident in data
- First-class passengers had higher survival rates
- Age affects survival chances
- Clear gender-based survival disparity

---

### Step 4: TRAIN
**Objective**: Build decision tree model

**Operations**:
- Select features: [Pclass, Sex, Age, SibSp, Parch, Embarked]
- Split data: 67% training (596 samples), 33% testing (295 samples)
- Encode categorical variables using LabelEncoder
- Initialize Decision Tree Classifier
- Fit model on training data

**Output**: Trained model with decision rules

---

### Step 5: TEST
**Objective**: Evaluate model performance

**Operations**:
- Make predictions on test set
- Calculate classification metrics (accuracy, precision, recall, F1)
- Generate confusion matrix
- Create actual vs predicted plots
- Analyze model performance

**Output**: Performance metrics and visualizations

---

### Step 6: DEPLOY
**Objective**: Make predictions on new passenger data

**Operations**:
- Accept passenger characteristics input
- Generate survival prediction
- Display prediction confidence
- Show prediction explanation

**Example Predictions**:
```
Example 1: Female, Class 1, Age 25
Prediction: SURVIVED (High confidence)

Example 2: Male, Class 3, Age 30
Prediction: DID NOT SURVIVE (Moderate confidence)

Example 3: Female, Class 2, Age 35
Prediction: SURVIVED (High confidence)
```

---

## üíª Technical Implementation

### Data Processing Pipeline

```python
# Load data
titanic_df = pd.read_csv(url)

# Handle missing Age values
age_by_class = titanic_clean.groupby('Pclass')['Age'].mean()
titanic_clean['Age'] = titanic_clean.apply(
    lambda row: age_by_class[row['Pclass']] if pd.isnull(row['Age']) else row['Age'],
    axis=1
)

# Handle missing Embarked values
embarked_mode = titanic_clean['Embarked'].mode()[0]
titanic_clean['Embarked'].fillna(embarked_mode, inplace=True)

# Encode categorical variables
label_encoder = LabelEncoder()
titanic_clean['Sex'] = label_encoder.fit_transform(titanic_clean['Sex'])
titanic_clean['Embarked'] = label_encoder.fit_transform(titanic_clean['Embarked'])
```

### Model Training

```python
# Prepare features and target
X = titanic_clean.drop('Survived', axis=1)
y = titanic_clean['Survived']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=3
)

# Train model
model = DecisionTreeClassifier(
    random_state=3,
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5
)
model.fit(X_train, y_train)
```

### Model Evaluation

```python
# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
```

---

## üìñ Algorithm Deep Dive

For a comprehensive understanding of Decision Tree Classifier, including:
- Mathematical formulas (entropy, information gain, Gini impurity)
- Why Decision Tree was chosen for Titanic survival prediction
- Step-by-step algorithm working process
- Learning perspective and assumptions
- Feature importance analysis
- Historical insights and decision rules

**See**: `ALGORITHM_DECISION_TREE.md`

---

## üéì Learning Outcomes

### For Students

By completing this project, you will learn:

1. **Classification Fundamentals**
   - Binary classification (survived/not survived)
   - Classification metrics (accuracy, precision, recall, F1)
   - Confusion matrix interpretation
   - Class imbalance handling

2. **Decision Trees**
   - How decision trees make predictions
   - Tree depth and overfitting
   - Feature importance from splits
   - Hyperparameter tuning

3. **Data Preprocessing**
   - Handling missing values strategically
   - Categorical variable encoding
   - Feature selection
   - Data quality assessment

4. **Exploratory Analysis**
   - Pattern discovery in data
   - Visualization interpretation
   - Statistical analysis
   - Business insights from data

5. **Model Evaluation**
   - Multiple evaluation metrics
   - Train/test splitting
   - Performance interpretation
   - Model comparison

---

## üîç Key Insights

### Survival Factors
- **Gender**: Strongest predictor (women had much higher survival rates)
- **Class**: First-class passengers had better survival chances
- **Age**: Younger passengers had higher survival rates
- **Family Size**: Traveling with family affected survival

### Historical Context
- "Women and children first" evacuation policy clearly evident
- Class-based inequality in survival outcomes
- Age bias in survival (children prioritized)
- Gender bias in evacuation procedures

### Model Insights
- Decision tree captures these patterns effectively
- Feature importance aligns with historical records
- Model achieves 78.64% accuracy on test data
- Good balance between precision and recall

---

## üêõ Troubleshooting

### Issue: "ModuleNotFoundError: No module named 'jupyter'"

**Solution**:
```bash
# Install Jupyter
pip install jupyter notebook

# Or reinstall all dependencies
pip install -r requirements.txt
```

### Issue: "Data loading fails"

**Solution**:
```bash
# Check internet connection
ping github.com

# Verify pandas is installed
pip install pandas==2.0.3
```

### Issue: "Matplotlib display error"

**Solution**:
```bash
# For Jupyter, use:
%matplotlib inline

# For Python script, ensure matplotlib is installed:
pip install matplotlib==3.7.2
```

---

## üìö Additional Resources

### Documentation
- [Scikit-learn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Jupyter Documentation](https://jupyter.org/documentation)

### Tutorials
- [Decision Tree Tutorial](https://scikit-learn.org/stable/modules/tree.html)
- [Classification Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Titanic Dataset Analysis](https://www.kaggle.com/c/titanic)

### Related Projects
- Advertising Analysis (Flask, Linear Regression)
- E-commerce Analysis (Streamlit, Linear Regression)

---

## üìù File Descriptions

### BITS_AIML_Titanic_Jan3rd2026.ipynb
Jupyter Notebook with interactive cells for learning:
- Step-by-step execution
- Inline visualizations
- Markdown documentation
- Experimental modifications

### BITS_AIML_Titanic_Jan3rd2026.py
Standalone Python script for batch processing:
- Complete pipeline execution
- Console output
- Visualization windows
- No notebook interface required

### requirements.txt
List of all Python dependencies with specific versions.

---

## üéØ Next Steps

### For Learners
1. Run the notebook and explore each cell
2. Modify hyperparameters and observe results
3. Experiment with different features
4. Try different train/test splits
5. Implement cross-validation

### For Developers
1. Implement ensemble methods (Random Forest, Gradient Boosting)
2. Add hyperparameter tuning (GridSearchCV)
3. Implement cross-validation
4. Create web API for predictions
5. Deploy to cloud platform

---

## ‚úÖ Checklist

Before running the application:
- [ ] Python 3.7+ installed
- [ ] Virtual environment created
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Internet connection available
- [ ] All files in correct directory
- [ ] Jupyter installed (for notebook) or Python (for script)

---

**Last Updated**: January 2026
**Version**: 1.0
**Status**: Production Ready
**Maintainer**: BITS Hackathon Team
